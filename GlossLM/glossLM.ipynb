{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval import Retriever\n",
    "from igt import IGT\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from transformers import T5ForConditionalGeneration, ByT5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bd30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to transform igt instances in a file into igt object. igt object is a class used for the retrieve method\n",
    "\n",
    "def parse_igt(file_path):\n",
    "\n",
    "    lines = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            stripped = line.strip()\n",
    "            if stripped:\n",
    "                lines.append(stripped)\n",
    "\n",
    "    igts = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        if lines[i].startswith(\"\\\\t\"):\n",
    "            transcription = lines[i][2:].strip()\n",
    "            if lines[i+1].startswith(\"\\\\m\"):               # open track file process\n",
    "                morpheme = lines[i+1][2:].strip()\n",
    "                gloss = lines[i+2][2:].strip()\n",
    "                translation = lines[i+3][2:].strip()\n",
    "                igts.append(IGT(\n",
    "                    transcription=transcription,\n",
    "                    glosses=gloss,\n",
    "                    morpheme=morpheme,\n",
    "                    translation=translation,\n",
    "                    language=\"Gitksan\",\n",
    "                    metalang=\"English\"\n",
    "                ))\n",
    "                i += 4\n",
    "\n",
    "            else:           # close track file process\n",
    "                gloss = lines[i+1][2:].strip()\n",
    "                translation = lines[i+2][2:].strip()\n",
    "                igts.append(IGT(\n",
    "                    transcription=transcription,\n",
    "                    glosses=gloss,\n",
    "                    morpheme=None,\n",
    "                    translation=translation,\n",
    "                    language=\"Gitksan\",\n",
    "                    metalang=\"English\"\n",
    "                ))\n",
    "                i += 3\n",
    "        else:\n",
    "            i += 1\n",
    "    return igts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "train_file = \"polygloss/data/raw/sigmorphon_st/Lezgi/lez-train-track2-uncovered\"   # change input file\n",
    "train_igts = parse_igt(train_file)\n",
    "\n",
    "train_files = [igt.__dict__ for igt in train_igts]      # transform into dict\n",
    "train_dataset = Dataset.from_list(train_files)  # transform into Hugging Face dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c7b72",
   "metadata": {},
   "source": [
    "Code for zero shot experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de363a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"lecslab/glosslm\")\n",
    "tokenizer = ByT5Tokenizer.from_pretrained(\"google/byt5-base\", use_fast=False)\n",
    "\n",
    "with open(\"polygloss/data/raw/sigmorphon_st/Lezgi/lez-test-track2-covered\", \"r\", encoding=\"utf-8\") as f:    # change file path\n",
    "    lines = f.readlines()\n",
    "\n",
    "igt_blocks = []\n",
    "cur_block = []\n",
    "\n",
    "for line in lines:          # store igt instances as single blocks\n",
    "    if line.startswith(\"\\\\t\"):\n",
    "        if cur_block:\n",
    "            igt_blocks.append(cur_block)\n",
    "        cur_block = [line]\n",
    "    else:\n",
    "        cur_block.append(line)\n",
    "if cur_block:\n",
    "    igt_blocks.append(cur_block)\n",
    "\n",
    "cleaned_blocks = []        # remove starting symbols\n",
    "for block in igt_blocks:\n",
    "    transcription = \"\"\n",
    "    morpheme = None\n",
    "    translation = \"\"\n",
    "    for line in block:\n",
    "        if line.startswith(\"\\\\t\"):\n",
    "            transcription = line[2:].strip()\n",
    "        elif line.startswith(\"\\\\m\"):\n",
    "            morpheme = line[2:].strip()\n",
    "        elif line.startswith(\"\\\\l\"):\n",
    "            translation = line[2:].strip()\n",
    "    if transcription and translation:\n",
    "        cleaned_blocks.append({\"transcription\": transcription, \"morphemes\": morpheme, \"translation\": translation})\n",
    "\n",
    "prompts = []        # prompts for GlossLM.\n",
    "\n",
    "for ex in cleaned_blocks:\n",
    "    if ex[\"morphemes\"]:     # for open track file\n",
    "\n",
    "        prompt = f\"\"\"Provide the glosses for the following segmented transcription in Lezgian.  # change language here\n",
    "\n",
    "Transcription in Gitxsan: {ex['morphemes']}\n",
    "Transcription segmented: true\n",
    "Translation in English: {ex['translation']}\n",
    "\n",
    "Glosses:\n",
    "\"\"\"\n",
    "    else:       # for closs track file\n",
    "        prompt = f\"\"\"Provide the glosses for the following transcription in Lezgian.    #change language\n",
    "\n",
    "Transcription in Gitxsan: {ex['transcription']}\n",
    "Transcription segmented: false\n",
    "Translation in English: {ex['translation']}\n",
    "\n",
    "Glosses:\n",
    "\"\"\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "batch_size = 4\n",
    "predicted_glosses = []\n",
    "\n",
    "\n",
    "for i in range(0, len(prompts), batch_size):        # predict\n",
    "    batch = prompts[i:i + batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_length=1024, num_beams=1)\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    predicted_glosses.extend(decoded)\n",
    "\n",
    "results = []        # final gloss predictions\n",
    "for block, gloss in zip(igt_blocks, predicted_glosses):\n",
    "    new_block = []\n",
    "    gloss_inserted = False\n",
    "    for line in block:\n",
    "        if line.startswith(\"\\\\g\"):\n",
    "            new_block.append(f\"\\\\g {gloss}\\n\")\n",
    "            gloss_inserted = True\n",
    "        else:\n",
    "            new_block.append(line)\n",
    "    results.extend(new_block)\n",
    "\n",
    "with open(\"git_open_noex.txt\", \"w\", encoding=\"utf-8\") as f:     # store glosses in a file\n",
    "    f.writelines(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf31df",
   "metadata": {},
   "source": [
    "Code for one shot experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3698e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever.stock(\"max_word_coverage\", n_examples=1, dataset=train_dataset, seed=42)  \n",
    "to_gloss = \"polygloss/data/raw/sigmorphon_st/Lezgi/lez-test-track2-covered\"\n",
    "test_igts = parse_igt(to_gloss)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"lecslab/glosslm\")\n",
    "tokenizer = ByT5Tokenizer.from_pretrained(\"google/byt5-base\", use_fast=False)\n",
    "\n",
    "prompts = []\n",
    "for test_igt in test_igts:\n",
    "    ex = retriever.retrieve(test_igt)[0]\n",
    "    prompt = f\"\"\"Here are some complete glossed examples:\n",
    "\n",
    "    \\t {ex.morpheme}                # change this line to ex.transcription in close track   \n",
    "    \\g {ex.glosses}\n",
    "    \\l {ex.translation}\n",
    "\n",
    "    Provide the glosses for the following transcription in Lezgian.     # change language\n",
    "\n",
    "    Transcription in Lezgian: {test_igt.morpheme}       # change to test_igt.transcription in close track\n",
    "    Transcription segmented: true                   # set to false in close track\n",
    "    Translation in English: {test_igt.translation}\n",
    "\n",
    "    Glosses:\n",
    "    \"\"\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "batch_size = 4\n",
    "predicted_glosses = []\n",
    "\n",
    "print(\"start glossing\")\n",
    "\n",
    "for i in range(0, len(prompts), batch_size):\n",
    "\n",
    "    batch = prompts[i:i + batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask,\n",
    "                             max_length=1024, num_beams=1)\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    predicted_glosses.extend(decoded)\n",
    "\n",
    "output_path = \"/content/lez_open_1ex.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for test_igt, gloss in zip(test_igts, predicted_glosses):\n",
    "        f.write(f\"\\\\t {test_igt.transcription}\\n\")\n",
    "        f.write(f\"\\\\m {test_igt.morpheme}\\n\")\n",
    "        f.write(f\"\\\\g {gloss}\\n\")\n",
    "        f.write(f\"\\\\l {test_igt.translation}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
